linked_means <- as.vector(X %*% as.matrix(beta))
logLogisticRegCndtlLike(y, linked_means)
}
logMissingProbs <- function(M, xmis, phis){
X[is.na(X[,2])] <- xmis
X <- cbind(rep(1,numRows), X)
stopifnot(is.matrix(M))
stopifnot(dim(M)[1] == numRows)
stopifnot(dim(M)[2] == 2)
stopifnot(sum(is.na(X))==0)
linked_means <- as.vector(X %*% as.matrix(phis))
logLogisticRegCndtlLike(M[,2], linked_means)
}
logMissingDataProbs <- function(xmis, alpha){
sum(dnorm(xmis, 0, sd = sqrt(alpha), log = T))
}
# log p(m|y, xmis, xobs, phi) + log p(y|xmis,xobs,beta) + log p(xmis|xobs,alpha) - log q(xmis|xobs,y,alpha,beta)
getOneLogSummandWithChosenX <- function(alpha, beta, phis, y, x1, x2){
Xmis <- propose_missdata_given_params(y, alpha)
Xmis$x2mis[1] <- x1
Xmis$x2mis[2] <- x2
logLikes <- logCondLikes(Xmis$x2mis, beta)
logpmweights <- logMissingProbs(M, Xmis$x2mis, phis)
logMissXProbs <- logMissingDataProbs(Xmis$x2mis, alpha)
logQs <- logMissingDataProposal(Xmis$x2mis, alpha)
sum(logLikes+logpmweights+logMissXProbs-logQs)
}
getLogImpSampEst <- function(alphaVec, betaVec, phiVec, x1, x2, N=100){
#samps <- replicate(N, getOneLogSummandWithChosenX(alphaVec, betaVec, phiVec, y, x1, x2))
samps <- getOneLogSummandWithChosenX(alphaVec, betaVec, phiVec, y, x1, x2)
matrixStats::logSumExp(samps-log(N))
}
#### consrruct the plot
# hat{p}(m,y, xmis | xobs, alpha, beta, phi)
plotThis <- function(xmis1, xmis2)
{
# alpha, beta, phi, xobs, y, m from above
# hardcoded N
N <- 10
logMissingDataProposal(c(xmis1, xmis2), alpha) + getLogImpSampEst(alpha, betas, phis, xmis1, xmis2, N)
}
plotSurface(-100, 100, -100, 100, 20, plotThis, F, theta=-120, zlab = "z", xlab = "x1", ylab = "x2")
betas
# sample q(x_{mis} | x_{obs}, y)
#TODO edit TeX and mention how you "neutralized scale bit" and how this
# is different from curen thinking
# maybe try to seek to explain why there is a "quicksand" effect
# with this particular thing going on
propose_missdata_given_params <- function(y, alpha){
list(x2mis = rt(numMissingEachGroup, df = 3))
#list(x2mis = rt(numMissingEachGroup, df = 10)*sqrt(alpha))
}
# eval log q(x_{mis} | x_{obs}, y)
logMissingDataProposal <- function(x2mis, alpha){
sum(dt(x = x2mis, df = 3, log = T))
#sum(dt.scaled(x = x2mis, df = 10, sd = sqrt(alpha), log = T))
}
logCondLikes <- function(xmis, beta){
# insert simulated data into missing data spots
X[is.na(X[,2])] <- xmis
X <- cbind(rep(1,numRows), X)
stopifnot(sum(is.na(X))==0)
linked_means <- as.vector(X %*% as.matrix(beta))
logLogisticRegCndtlLike(y, linked_means)
}
logMissingProbs <- function(M, xmis, phis){
X[is.na(X[,2])] <- xmis
X <- cbind(rep(1,numRows), X)
stopifnot(is.matrix(M))
stopifnot(dim(M)[1] == numRows)
stopifnot(dim(M)[2] == 2)
stopifnot(sum(is.na(X))==0)
linked_means <- as.vector(X %*% as.matrix(phis))
logLogisticRegCndtlLike(M[,2], linked_means)
}
logMissingDataProbs <- function(xmis, alpha){
sum(dnorm(xmis, 0, sd = sqrt(alpha), log = T))
}
# log p(m|y, xmis, xobs, phi) + log p(y|xmis,xobs,beta) + log p(xmis|xobs,alpha) - log q(xmis|xobs,y,alpha,beta)
getOneLogSummandWithChosenX <- function(alpha, beta, phis, y, x1, x2){
Xmis <- propose_missdata_given_params(y, alpha)
Xmis$x2mis[1] <- x1
Xmis$x2mis[2] <- x2
logLikes <- logCondLikes(Xmis$x2mis, beta)
logpmweights <- logMissingProbs(M, Xmis$x2mis, phis)
logMissXProbs <- logMissingDataProbs(Xmis$x2mis, alpha)
logQs <- logMissingDataProposal(Xmis$x2mis, alpha)
sum(logLikes+logpmweights+logMissXProbs-logQs)
}
getLogImpSampEst <- function(alphaVec, betaVec, phiVec, x1, x2, N=100){
#samps <- replicate(N, getOneLogSummandWithChosenX(alphaVec, betaVec, phiVec, y, x1, x2))
samps <- getOneLogSummandWithChosenX(alphaVec, betaVec, phiVec, y, x1, x2)
matrixStats::logSumExp(samps-log(N))
}
#### consrruct the plot
# hat{p}(m,y, xmis | xobs, alpha, beta, phi)
plotThis <- function(xmis1, xmis2)
{
# alpha, beta, phi, xobs, y, m from above
# hardcoded N
N <- 10
logMissingDataProposal(c(xmis1, xmis2), alpha) + getLogImpSampEst(alpha, betas, phis, xmis1, xmis2, N)
}
plotSurface(-100, 100, -100, 100, 20, plotThis, F, theta=-120, zlab = "z", xlab = "x1", ylab = "x2")
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
y
numMissingEachGroup
numColsWithMiss <- sum(colSums(mis) != 0)
M
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
numMissingEachGroup
numRows <- 4
betas <- c(1, -2, 3)
alpha <- 10.1
phis <- c(1, 1, 1)
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
?persp
plotThis(0,0)
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-100,-10))
plotThis(0,0)
plotThis(-100,-100)
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-500,-10))
plotThis(-1000,-1000)
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-11000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-15000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-14000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-20000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-18000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-15000,-10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-16000,-10))
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
#' @param lowerSecond lower bound of y axis
#' @param upperSecond upper bound of y axis
#' @param numGridPointsOnEachAxis how many grid points do you want on each axis
#' @param f the function that takes two scalar arguments (x and y) and produces one scalar argument (z)
#' @param contour do you want a contour plot? (True or False)
#' @param ... extra arguments to be passed to graphics::contour() or graphics::persp() (depending on what contour arg was set to)
#' @keywords plotting 3D 3-D 3d
#' @export
#' @examples
#' plotSurface(-50, 50, 0.0001, 50, 20, eval_log_unnormalized_posterior, F, theta=-120, zlab = "log unnorm dens", xlab = "mu", ylab = "ss")
plotSurface <- function(lowerFirst, upperFirst, lowerSecond, upperSecond,
numGridPointsOnEachAxis, f, contour = F, ...)
{
A <- seq(lowerFirst, upperFirst, length.out = numGridPointsOnEachAxis)
B <- seq(lowerSecond, upperSecond, length.out = numGridPointsOnEachAxis)
args <- expand.grid(A,B)
z <- mapply(f, args[,1], args[,2])
dim(z) <- c(length(A), length(B))
if(contour){
graphics::contour(A, B, z, ...)
}else{
graphics::persp(x=A, y=B, z=z, ...)
}
}
source('~/pmglm/code/plot_fake_data_sim_like.r')
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(0,10))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2")
source('~/pmglm/code/plot_fake_data_sim_like.r')
source('~/pmglm/code/plot_fake_data_sim_like.r')
plotThis(1,1)
plotThis(-100,-100)
plotThis(0,0)
#### consrruct the plot
# hat{p}(m,y, xmis | xobs, alpha, beta, phi)
plotThis <- function(xmis1, xmis2)
{
# alpha, beta, phi, xobs, y, m from above
# hardcoded N
N <- 1
logMissingDataProposal(c(xmis1, xmis2), alpha) + getLogImpSampEst(alpha, betas, phis, xmis1, xmis2, N)
}
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2")
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-15000,-15))
source('~/pmglm/code/plot_fake_data_sim_like.r')
plotThis(-1000,-1000)
plotThis(0,0)
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-15000,0))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-150000,0))
plotSurface(-1000, 1000, -1000, 1000, 30,
plotThis, F, theta=-120,
zlab = "z", xlab = "x1", ylab = "x2",
zlim = c(-1500000,0))
x <- seq(-10,30)
y <- 3 - 2*x +
arima.sim(n = length(x),
list(ar = c(0.8897)),
sd = 3)
plot(x,y)
abline(lm(y ~ x))
plot(residuals(lm(y ~ x)), type ="b")
abline(h=0,col="red")
knitr::include_graphics("durbin_cutoffs.png")
suppressMessages(library(lmtest))
dwtest(lm(y ~ x), alternative = "greater") # need library(lmtest)
# don't change order=
# because we haven't
# discussed any other
# time series models
corrErrorMod <-
arima(x=y, #dependent variable has a confusing name
xreg=x, #independent variable
include.mean = T,
method="ML",
order=c(1,0,0))
plot(residuals(corrErrorMod)) #these look better
summary(corrErrorMod)
corrErrorMod$coef # phi, beta0, beta1
summary(lm(y ~ x)) # estimates are pretty close to simple ols model
knitr::include_graphics("sample_with_replacement.png")
n <- 50
true_beta0 <- -2
true_beta1 <- 3
true_sigma_squared <- 5
x <- seq(0,100, length.out = n)
y <- true_beta0 + true_beta1*x + rnorm(n = n, mean = 0, sd = sqrt(true_sigma_squared))
random_indexes <- sample.int(length(x), replace=TRUE) # need replace=T
sort(random_indexes) # just to show that some are repeating
xstar <- x[random_indexes]
ystar <- y[random_indexes]
coefficients(lm(ystar ~ xstar))
bootstrap_once <- function(){
random_indexes <- sample.int(length(x), replace=TRUE) # need replace=T
xstar <- x[random_indexes]
ystar <- y[random_indexes]
return(coefficients(lm(ystar ~ xstar))[1])
}
m <- 1e3
beta0Stars <- replicate(m, bootstrap_once())
hist(beta0Stars)
abline(v=true_beta0, col = "red") # true unknown
abline(v=coefficients(lm(y ~ x))[1], col ="green") # estimated from our sample
abline(v=quantile(beta0Stars, .025), col = "blue") # bootstrap lower
abline(v=quantile(beta0Stars, .975), col = "blue") # bootstrap upper
confint(lm(y ~ x)) # non bootstrapped intervals
original_beta_hat <- coefficients(lm(y ~ x))[1]
bootstrap_resids_once <- function(){
random_indexes <- sample.int(length(x), replace=TRUE) # need replace=T
xstar <- x[random_indexes]
ystar <- y[random_indexes]
return(coefficients(lm(ystar ~ xstar))[1])
}
m <- 1e3
beta0Stars <- replicate(m, bootstrap_once())
hist(beta0Stars)
abline(v=true_beta0, col = "red") # true unknown
abline(v=coefficients(lm(y ~ x))[1], col ="green") # estimated from our sample
abline(v=quantile(beta0Stars, .025), col = "blue") # bootstrap lower
abline(v=quantile(beta0Stars, .975), col = "blue") # bootstrap upper
confint(lm(y ~ x)) # non bootstrapped intervals
original_beta_hat <- coefficients(lm(y ~ x))[1]
original_beta_hat
residuals(lm(y ~ x))
model.matrix(x)
model.matrix(as.matrix(x))
x
matrix(rep(1,length(x)), x)
length(x)
matrix(rep(1,length(x)))
matrix(c(rep(1,length(x)),residuals(lm(y ~ x))))
matrix(c(rep(1,length(x)),residuals(lm(y ~ x))), ncol=2)
original_beta_hat <- coefficients(lm(y ~ x))
original_beta_hat
as.matrix(original_beta_hat)
original_beta_hat <- as.matrix(coefficients(lm(y ~ x)))
original_beta_hat
original_beta_hat <- as.matrix(coefficients(lm(y ~ x)))
original_residuals <- residuals(lm(y ~ x))
X <- matrix(c(rep(1,length(x)), x), ncol=2)
random_indexes <- sample.int(length(x), replace=TRUE) # need replace=T
estar <- original_residuals[random_indexes]
ystar <- X %*% original_beta_hat + estar
ystar
tail(y)
head(longley)
MASS
??MASS
?lm.ridge
library(MASS)
?lm.ridge
head(longley)
?plot.lm.ridge
?plot.ridge
?plot(mod)
names(mod)
attr(mod)
library(MASS) # for lm.ridge()
head(longley)
mod <- lm.ridge(GNP.deflator ~ ., longley, lambda = seq(0,0.1,0.001)) #k = lambda
attr(mod)
class(mod)
plot.ridgelm
?plot.ridge
mod
names(mod)
?lm.ridge
coef(mod)
plot(mod, type = "vif")
plot(mod, type = "ridge")
?lmridge
??lmridge
#library(MASS) # for lm.ridge()
library(lmridge)
install.packages("lmridge")
#library(MASS) # for lm.ridge()
library(lmridge)
head(longley)
mod <- lmridge(GNP.deflator ~ ., longley, lambda = seq(0,0.1,0.001)) #k = lambda
plot(mod)
plot(mod, type = "ridge")
?plot.lmridge
#library(lmridge) #probably need to install this first
library(glmnet)
?glmnet
?plot.glmnet
head(longley)
as.matrix(longley[,-1])
as.vector(longley[,1])
mod <- glmnet(as.matrix(longley[,-1]), # X matrix
as.vector(longley[,1]), # y vector
family = "gaussian", # don't change this
alpha = 0) # don't change this
mod
plot(mod)
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda")
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, type = "ridge")
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod)
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda")
?plot.glmnet
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda", label = T, type.coef = "coef")
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda", type.coef = "coef")
#library(lmridge) #probably need to install this first
library(glmnet)
head(longley)
mod <- glmnet(as.matrix(longley[,-1]), # X matrix
as.vector(longley[,1]), # y vector
family = "gaussian", # don't change this
alpha = 0) # don't change this
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda", type.coef = "coef")
knitr::include_graphics("bell_curves.png")
#library(lmridge) #probably need to install this first
library(glmnet)
head(longley)
mod <- glmnet(as.matrix(longley[,-1]), # X matrix
as.vector(longley[,1]), # y vector
family = "gaussian", # don't change this
alpha = 0) # don't change this
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot(mod, xvar = "lambda", type.coef = "coef")
plot(mod, type = "ridge")
?plot.glmnet
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot.glmnet(mod, xvar = "lambda", type.coef = "coef")
install.packages(c("dplyr", "ellipsis", "ggplot2", "git2r", "markdown", "mime", "mvtnorm", "numDeriv", "pillar", "quantreg", "RcppArmadillo", "remotes", "rlang", "servr", "tibble", "tinytex", "xfun"))
install.packages(c("dplyr", "ellipsis", "ggplot2", "git2r", "markdown", "mime", "mvtnorm", "numDeriv", "pillar", "quantreg", "RcppArmadillo", "remotes", "rlang", "servr", "tibble", "tinytex", "xfun"))
install.packages(c("dplyr", "ellipsis", "ggplot2", "git2r", "markdown", "mime", "mvtnorm", "numDeriv", "pillar", "quantreg", "RcppArmadillo", "remotes", "rlang", "servr", "tibble", "tinytex", "xfun"))
install.packages(c("dplyr", "ellipsis", "ggplot2", "git2r", "markdown", "mime", "mvtnorm", "numDeriv", "pillar", "quantreg", "RcppArmadillo", "remotes", "rlang", "servr", "tibble", "tinytex", "xfun"))
#library(lmridge) #probably need to install this first
library(glmnet)
head(longley)
mod <- glmnet(as.matrix(longley[,-1]), # X matrix
as.vector(longley[,1]), # y vector
family = "gaussian", # don't change this
alpha = 0) # don't change this
# mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
#plot(mod, type = "ridge", abline = T) # fyi ?plot.lmridge
plot.glmnet(mod, xvar = "lambda", type.coef = "coef")
plot.glmnet
?plotCoef
plotCoef
?plot.raster
library(lmridge) #probably need to install this first
?plot.raster
?plot.lmridge
?lm.ridge
?lmridge
library(lmridge) #probably need to install this first
head(longley)
head(longley)
mod <- lmridge(GNP.deflator ~ ., longley, K = seq(0,0.05,0.001)) # capital K
plot(mod, type = "ridge", abline = F) # fyi ?plot.lmridge
mod$Z
?glmnet
library(glmnet)
head(longley)
X <- model.matrix( ~ . - GNP.deflator, data = longley)
lambdas <- seq(0, 1, .01)
mod <- glmnet(x = X,
y = as.matrix(longley[,1]),
family = "gaussian",
alpha = "0", # don't change
lambda = lambdas,
standardize = T, # this is the default
intercept = T)
#plot(mod, type = "ridge", abline = F) # fyi ?plot.lmridge
plot(mod)
#plot(mod, type = "ridge", abline = F) # fyi ?plot.lmridge
plot(mod,xvar = "lambda")
#plot(mod, type = "ridge", abline = F) # fyi ?plot.lmridge
plot(mod, xvar = "lambda")
library(glmnet)
head(longley)
X <- model.matrix( ~ . - GNP.deflator, data = longley)
possible_k <- seq(0, 100, .1)
mod <- glmnet(x = X,
y = as.matrix(longley[,1]),
family = "gaussian",
alpha = "0", # don't change
lambda = possible_k,
standardize = T, # this is the default
intercept = T)
#plot(mod, type = "ridge", abline = F) # fyi ?plot.lmridge
plot(mod, xvar = "lambda")
?plot.glmnet
mod <- glmnet(x = X,
y = as.matrix(longley[,1]),
family = "gaussian",
alpha = "0", # don't change
lambda = possible_k,
standardize = F, # this is the default
intercept = T)
plot(mod, xvar = "lambda", label=T)
mod <- glmnet(x = X,
y = as.matrix(longley[,1]),
family = "gaussian",
alpha = "0", # don't change
lambda = possible_k,
standardize = T, # this is the default
intercept = T)
?cv.glmnet
nrow(longley)
?plot.cv.glmnet
mod2 <- cv.glmnet(x = X,
y = as.matrix(longley[,1]),
family = "gaussian",
alpha = "0", # don't change this
standardize = T, # default is T
intercept = T,
nfolds = 3)
names(mod2)
mod2$lambda.min
mod2$lambda.1se
setwd("~/UVa/all_teaching/summer19_6021/presentations/")
library(rmarkdown)
rmarkdown::render_site()
