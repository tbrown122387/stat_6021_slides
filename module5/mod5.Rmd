---
title: Module 5
subtitle: STAT 6021
author: Taylor R. Brown, PhD
output: slidy_presentation
---


## Exploring $\mathbf{H}$

When are predictions high-variance?
$$
V[\hat{\mathbf{y}}] = \sigma^2 \mathbf{H}
$$
So in particular 
$$
V[\hat{y}_i] = \sigma^2 \mathbf{H}_{ii}
$$

This has to do with "extrapoloating" (more on this later)


## Exploring $\mathbf{H}$

When are predictions high-variance?
$$
V[\mathbf{e}] = \sigma^2 [\mathbf{I} - \mathbf{H}]
$$
So in particular 
$$
V[e_i] = \sigma^2(1- \mathbf{H}_{ii})
$$

high leverage points mean that we predict with full certainty (not believable!)


## Exploring $\mathbf{H}$

Recall 
$$
\mathbf{H} \mathbf{X} = \mathbf{X}.
$$

Looking at the first column 
$$
\mathbf{H} \mathbf{1} = \mathbf{1}
$$

So row-sums of the hat matrix are equal to $1$. We know

$$
\hat{y}_3 = \sum_{i=1}^n y_i h_{i,3}
$$
after looking at the $i$th row of $\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}$. 

So if $h_{ii} \approx 1$ then the $i$th observation has a lot of *leverage* in determing the $i$th prediction.

## Leverage

Recall 
$$
\mathbf{H} =  \mathbf{H}^2 
$$
So
$$
\mathbf{H}_{ii} =  \sum_{l \neq i} \mathbf{H}_{i,l}^2 + \mathbf{H}_{ii}^2  \ge \mathbf{H}_{ii}^2 \ge 0
$$
So 
$$
0 \le \mathbf{H}_{ii} \le 1.
$$

Rule of thumb: flag row $i$ when $\mathbf{H}_{ii} > 3 \frac{p}{n}$

<!-- ## Exploring $\mathbf{H}$ -->


## Exploring $\mathbf{H}$

Describing *leverage*...

How far away is the $i$th row/observation from the typical observation?

I.e. how far is $\mathbf{x}_i^{\intercal} = (1, x_{i1}, x_{i2}, \ldots, x_{ik})$ from $\bar{x}^{\intercal}$

(Squared) Euclidean distance?

$$
[1-1]^2 + [x_{i1} - \bar{x}_{1}]^2 + \cdots [x_{ik} - \bar{x}_{k}]^2
$$

If our model has an intercept, this can be written as
$$
(\mathbf{x}_i - \bar{\mathbf{x}})^{\intercal}(\mathbf{x}_i - \bar{\mathbf{x}})
$$



## Exploring $\mathbf{H}$

How far away is the $i$th row/observation from the typical observation?

This is better

$$
\frac{[x_{i1} - \bar{x}_{1}]^2}{ s_1^2 } + \cdots + \frac{[x_{ik} - \bar{x}_{k}]^2}{s_k^2 }
$$

This can be written as
$$
(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})^{\intercal}
\begin{bmatrix}
s_1^{2} & \cdots & 0\\
\vdots & \ddots & \vdots  \\
0 & \cdots  & s_k^2
\end{bmatrix}^{-1}
(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})
$$

## Exploring $\mathbf{H}$

How far away is the $i$th row/observation from the typical observation?


In general we have *Mahalanobis distance*
$$
(\mathbf{x}_{i,-1} - \bar{\mathbf{x}})^{\intercal}
\mathbf{S}^{-1}
(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})
$$
where 
$$
\mathbf{S} = 
\begin{bmatrix}
s_1^{2} & \cdots & s_{1k}\\
\vdots & \ddots & \vdots  \\
s_{k1} & \cdots  & s_k^2
\end{bmatrix}
$$
e.g. $s_1^2 = \sum_{i}(x_{i1} - \bar{x}_1)^2/(n-1)$


## Exploring $\mathbf{H}$

How does this relate to 
$$
\mathbf{H} = \mathbf{X}(\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}
$$
Each element of the hat matrix kind of looks like a Mahalanobis distance:
$$
\mathbf{H}_{i,j} = \mathbf{X}_i^{\intercal}(\mathbf{X}^{\intercal}\mathbf{X})^{-1}\underbrace{\mathbf{X}_j}_{\text{row j}}
$$


## Exploring $\mathbf{H}$

They're not exactly the same. Recall 
$$
\mathbf{X}^\intercal \mathbf{X} = 
\pmatrix{n & n\bar{\mathbf{x}}_{-1}^{\intercal} \\ n\bar{\mathbf{x}}_{-1} & \mathbf{C}}
$$
where $\mathbf{C}_{jk} = \sum_{i=1}^n x_{ij} x_{ik}$. The [inverse of this block matrix](https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion) is

$$
\begin{bmatrix} 1/n+\bar{\mathbf{x}}_{-1}^{\intercal}( \mathbf{C}- n\bar{\mathbf{x}}_{-1}\bar{\mathbf{x}}_{-1}^{\intercal})^{-1}\bar{\mathbf{x}}_{-1}  & -\bar{\mathbf{x}}_{-1}^{\intercal}( \mathbf{C}- n\bar{\mathbf{x}}_{-1}\bar{\mathbf{x}}_{-1}^{\intercal})^{-1} \\ - ( \mathbf{C}- n\bar{\mathbf{x}}_{-1}\bar{\mathbf{x}}_{-1}^{\intercal})^{-1}\bar{\mathbf{x}}_{-1} & ( \mathbf{C}- n\bar{\mathbf{x}}_{-1}\bar{\mathbf{x}}_{-1}^{\intercal})^{-1} \end{bmatrix}
$$
which equals 
$$
\begin{bmatrix} 1/n+\bar{\mathbf{x}}_{-1}^{\intercal}( [n-1] \mathbf{S}^2)^{-1}\bar{\mathbf{x}}_{-1}  & -\bar{\mathbf{x}}_{-1}^{\intercal}( [n-1] \mathbf{S}^2)^{-1} \\ - ([n-1] \mathbf{S}^2)^{-1}\bar{\mathbf{x}}_{-1} & ( [n-1] \mathbf{S}^2)^{-1} \end{bmatrix}
$$



## Exploring $\mathbf{H}$

So

\begin{align*}
\mathbf{H}_{i,i} &= \mathbf{X}_i^{\intercal}(\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}_i \\
&= 
\begin{bmatrix}
1 & \mathbf{x}_{i,-1}^\intercal
\end{bmatrix}
\begin{bmatrix} 1/n+\bar{\mathbf{x}}_{-1}^{\intercal}( [n-1] \mathbf{S}^2)^{-1}\bar{\mathbf{x}}_{-1}  & -\bar{\mathbf{x}}_{-1}^{\intercal}( [n-1] \mathbf{S}^2)^{-1} \\ - ([n-1] \mathbf{S}^2)^{-1}\bar{\mathbf{x}}_{-1} & ( [n-1] \mathbf{S}^2)^{-1} \end{bmatrix}
\begin{bmatrix}
1 \\
\mathbf{x}_{i,-1}
\end{bmatrix}\\
&= \begin{bmatrix}
\frac{1}{n} - (\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})^{\intercal}( [n-1] \mathbf{S}^2)^{-1}\bar{\mathbf{x}}_{-1}  & 
(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})^{\intercal}( [n-1] \mathbf{S}^2)^{-1}
\end{bmatrix}
\begin{bmatrix}
1 \\
\mathbf{x}_{i,-1}
\end{bmatrix}\\
&= \frac{1}{n}   + \underbrace{(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})^{\intercal}( \mathbf{S}^2)^{-1}(\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})}_{\text{Mahalanobis dist.} }/(n-1)
\end{align*}


Also describe variance of betas