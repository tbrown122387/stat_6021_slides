---
title: Module 8
subtitle: STAT 6021
author: Taylor R. Brown, PhD
output: slidy_presentation
---

## Motivation

Recall from module 5:

$$
\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}
$$

and if you look at one of the rows you have 

$$
\hat{y}_j = \sum_{i=1}^n y_i \mathbf{H}_{j,i}.
$$

The number $\mathbf{H}_{j,i}$ is the *leverage* of obs. $i$ on fitted value $j$. 

## Motivation 

In module 5 we discussed how "leverage" is related to distance form the center in $\mathbf{X}$-space:

$$
\mathbf{H}_{i,j} = \frac{1}{n}   + (\mathbf{x}_{i,-1} - \bar{\mathbf{x}}_{-1})^{\intercal}( \mathbf{S}^2)^{-1}(\mathbf{x}_{j,-1} - \bar{\mathbf{x}}_{-1}) /(n-1)
$$
where $\mathbf{S}$ is the matrix of sample variances/covariances (with $n-1$ in the denominator).

- $\mathbf{H}_{i,j}$ is the similarity between observations $i$ and $j$

- $\mathbf{H}_{i,i}$ is the distance of observation $i$ from the center.

## Definitions

In addition to leverage, we have the concept of an observation's *influence*. An observation has high influence if its existence affects the regression coefficient estimates.

```{r, echo=F, out.width="600px"}
knitr::include_graphics("leverage_and_influence.png")
```

## Cook's D

*Cook's D* can be thought of as a measure for how a particular observation affects the overall fit/coefficient estimates/predictions.

$$
D_i := \frac{\left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right)^\intercal (\mathbf{X}^\intercal \mathbf{X}) \left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right) }{p MS_{Res} }
$$
This is clearly the same as 
$$
D_i = \frac{\left( \hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}}\right)^\intercal \left( \hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}}\right) }{p MS_{Res} }
$$

Notice the last numerator is the PRESS statistic!


## Cook's D

Finally, Cook's D can be written as

$$
D_i = \frac{r_i^2}{p} \frac{\mathbf{H}_{ii}}{1 - \mathbf{H}_{ii}}
$$
where
$$
r_i = \frac{e_i}{\sqrt{ MS_{Res} (1 - \mathbf{H}_{ii}) }} 
$$
(defined in module 4). Influence is a combination of x-outliers (leverage) and y-outliers (residuals)!

## Cook's D

Proof: first, recall from module 7

$$
\hat{\boldsymbol{\beta}}_{(i)} = \hat{\boldsymbol{\beta}} - \frac{e_i (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{x}_i }{1-\mathbf{H}_{ii}}
$$

so
$$
\left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right)^\intercal (\mathbf{X}^\intercal \mathbf{X}) \left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right) = \frac{e_i^2 \mathbf{x}_i^\intercal(\mathbf{X}^\intercal \mathbf{X})^{-1}\mathbf{x}_i   }{(1-\mathbf{H}_{ii})^2}
$$

so
$$
D_i := \frac{\left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right)^\intercal (\mathbf{X}^\intercal \mathbf{X}) \left(\boldsymbol{\beta}_{(i)} - \boldsymbol{\beta}\right) }{p MS_{Res} } = \frac{e_i^2}{MS_{Res }(1-\mathbf{H}_{ii}) } \frac{ \mathbf{H}_{ii}   }{p (1-\mathbf{H}_{ii})} = \frac{r_i^2}{p} \frac{\mathbf{H}_{ii}}{1 - \mathbf{H}_{ii}}
$$

## Plotting Cook's D

```{r, echo=F}
data("mtcars")
carsdf <- mtcars[,-c(8:11)]
```

Cook's Distance doesn't have a nice sampling distribution (roughly $F$-distributed). Care if an observation has one above $1$.

```{r, echo=T, out.width="600px"}
fullMod <- lm(mpg ~ ., data = carsdf)
# plot(fullMod) # will allow you to cycle through afew plots interactively!
plot(fullMod, which=5)
```


## DFFITS and DFBETAS

Cook's D measures overall influence of each observation.

DFFITS and DFBETAS measure influence of each observation **on a particular coefficient, or a particular predicted value**.
